{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Este notebook muestra como utilizar el modelo **gpt2-small-spanish** para generación de lenguaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de librerías y preparación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead, set_seed\n",
    "from torch.cuda import set_device, current_device\n",
    "#from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las librerías necesarias para poder ejecutar el ejemplo que te presentamos son _transformers_, y _pytorch_. Puedes ver en la salida de la siguiente celda las versiones que fueron utilizadas para correr este ejemplo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.7.9\n",
      "transformers: 4.2.0.dev0\n",
      "pytorch: 1.7.1\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "from importlib_metadata import version\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"transformers:\", version(\"transformers\"))\n",
    "print('pytorch:', version('torch'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de importar las ilbrerías, asignamos el GPU que querramos utilizar para correr el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda device: 0\n"
     ]
    }
   ],
   "source": [
    "gpu = 0\n",
    "set_device(gpu)\n",
    "print(f'cuda device: {current_device()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar este ejemplo, utilizamos una tarjeta GeForce GTX1080Ti (aparecen cuatro pero asignamos solo la primera, con índice 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 15 00:59:17 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.27.04    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 19%   30C    P0    58W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 18%   32C    P0    59W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:83:00.0 Off |                  N/A |\n",
      "| 17%   31C    P0    57W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:84:00.0 Off |                  N/A |\n",
      "| 18%   30C    P0    58W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cargamos el tokenizer y el modelo que se encuentran alojados en la plataforma de huggingface. Para esto utilizamos la función de _from\\_pretrained_, utilizando como único parametro el nombre del modelo en un string \"datificate/gpt2-small-spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"datificate/gpt2-small-spanish\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"datificate/gpt2-small-spanish\")\n",
    "set_seed(1212) #para asegurar repetibilidad de los ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el modelo a la GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de unicornios\n",
    "\n",
    "Uno de los famosos ejemplos utilizados en la <a href=\"https://openai.com/blog/better-language-models/\" >presentación del modelo GPT2</a> es la generación de texto de unicornios. Aquí utilizamos una versión traducida al español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_unicornios = f'En un hallazgo impactante, el científico descubrió un rebaño de unicornios que vivían en un valle remoto, anteriormente inexplorado, ' \\\n",
    "                   f'en las montañas de los Andes. Aún más sorprendente para los investigadores fue el hecho de que los unicornios hablaban perfecto español.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "largo_maximo = 250\n",
    "largo_minimo = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos el texto a tokens, que son representaciones numéricas del texto utilizando el vocabulario ene español con el que el modelo fue entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identificadores de los tokens:\n",
      " tensor([[  507,   298, 18617, 10881,   840,    12,   284,  6548,  7223,   298,\n",
      "         13719,   768,   258,   298, 39113,   496,   301, 10166,   278,   298,\n",
      "          4654, 27646,    12,  5144, 11090, 40911,   326,    12,   278,   347,\n",
      "          6674,   258,   312, 10323,    14, 22232,   436, 15973,   372,   312,\n",
      "          7651,   405,   284,  1633,   258,   301,   312,   298, 39113,   496,\n",
      "         24085, 14771,  1119,    14]], device='cuda:0')\n",
      "\n",
      "Texto reconstruido:\n",
      " En un hallazgo impactante, el científico descubrió un rebaño de unicornios que vivían en un valle remoto, anteriormente inexplorado, en las montañas de los Andes. Aún más sorprendente para los investigadores fue el hecho de que los unicornios hablaban perfecto español.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(texto_unicornios, return_tensors='pt').to('cuda')\n",
    "\n",
    "texto_reconstruido = tokenizer.decode(tokenizer.encode(texto_unicornios))\n",
    "\n",
    "print('Identificadores de los tokens:\\n', input_ids)\n",
    "print('\\nTexto reconstruido:\\n', texto_reconstruido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Texto Generado 1\n",
      "\n",
      "En un hallazgo impactante, el científico descubrió un rebaño de unicornios que vivían en un valle remoto, anteriormente inexplorado, en las montañas de los Andes. Aún más sorprendente para los investigadores fue el hecho de que los unicornios hablaban perfecto español.\n",
      "\n",
      "A su vez, el descubrimiento de la especie \"N. scelanoides\" había llevado al editor de ciencia de la Universidad de Toronto, William King, a la conclusión de que \"nadie está de acuerdo en quién es el \"Unicornio del Pacífico Norte del mundo\". En consecuencia, King, en su artículo de 1996 \"La hipótesis de King y las dificultades de la hipótesis de King\", sugirió que el nombre del género \"Aeolophora\" no debe ser confundido con las lenguas de los indígenas de Norteamérica.\n",
      "\n",
      "En 1996, King propuso un nuevo género sinonimizado, \"Aeolophora alba\", que resultó ser en una revisión bastante amplia a partir de un estudio inicial del género \"Aeolophora\", que incluía a muchas especies diferentes. En el momento en que este trabajo fue publicado, King quería probar la hipótesis deKing con el material genético de \"Aeolophora\", y pensó que lo que\n",
      "\n",
      "---\n",
      ">> Texto Generado 2\n",
      "\n",
      "En un hallazgo impactante, el científico descubrió un rebaño de unicornios que vivían en un valle remoto, anteriormente inexplorado, en las montañas de los Andes. Aún más sorprendente para los investigadores fue el hecho de que los unicornios hablaban perfecto español. La teoría es que estos palabras no estaban relacionados con alguna lengua indígena, como por ejemplo se creía, con los unicornios. Además, los animales en cuestión eran una mezcla de los humanos y los los unicornios, de tal manera que los dexógrafos, si bien creían que estos animales vivían en el terreno, en el lugar se les conocía como \"paredarios de los unicornios\". En cambio, los dexógrafos creían que el rebaño de unicornios era de los mamíferos, ya que tenían un cráneo alargado y un área de distribución grande para la especie.\n",
      "\n",
      "Los primeros estudios de los unicornios fueron realizados por Etienne Baud, un paleontólogo aficionado, que tenía estudios de insectos, anfibios y reptiles. En 1896, Baud realizó una extensa descripción de los unicornios. El estudio fue más profundo, y su estudio demostró la existencia de un rebaño de una raza que habitaba una región, no obstante su tamaño reducido.\n",
      "\n",
      "---\n",
      ">> Texto Generado 3\n",
      "\n",
      "En un hallazgo impactante, el científico descubrió un rebaño de unicornios que vivían en un valle remoto, anteriormente inexplorado, en las montañas de los Andes. Aún más sorprendente para los investigadores fue el hecho de que los unicornios hablaban perfecto español. A lo largo de la historia del género, han vivido entre 6.000 y 8.000 años. Los noicornios son los únicos en alcanzar el estatus de hombre de la raza humana y la primera en conocer que el habla y la comunicación son muy avanzados.\n",
      "\n",
      "El término de origen del género se utiliza normalmente en una serie de contextos específicos para la selección. En esta lista se usa normalmente para la ciencia y el arte.\n",
      "\n",
      "El origen de la palabra \"unicornio\" se remonta hasta el siglo VI al menos a los yacimientos del paleolítico en el valle del río Anserma. En el libro \"El viaje al oeste de Asia\", Paul B. Watson publicó en 1899 un fragmento de la novela de H. R. G. Wells \"El viaje al sur de África\". Esta novela, aunque publicada en una época, se ha perdido en favor de una publicación en la segunda mitad del siglo XIX.\n",
      "\n",
      "En los siglos XVII y XVIII,\n",
      "\n",
      "---\n",
      "CPU times: user 4.72 s, sys: 97.3 ms, total: 4.81 s\n",
      "Wall time: 4.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#set top_k = 40 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(input_ids, pad_token_id=0, #50256,\n",
    "                                   do_sample=True, \n",
    "                                   max_length=largo_maximo, \n",
    "                                   min_length=largo_minimo,\n",
    "                                   top_k=40,\n",
    "                                   num_return_sequences=3)\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\">> Texto Generado {}\\n\\n{}\".format(i+1, tokenizer.decode(sample_output.tolist())))\n",
    "    print('\\n---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo sobre el coronavirus\n",
    "\n",
    "Para mostrar un ejemplo más, generamos varios textos hablando sober el coronavirus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_corona = f'Es importante tener en cuenta las medidas de higiene en tiempos de pandemia por coronavirus. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identificadores de los tokens:\n",
      " tensor([[  507,   298, 18617, 10881,   840,    12,   284,  6548,  7223,   298,\n",
      "         13719,   768,   258,   298, 39113,   496,   301, 10166,   278,   298,\n",
      "          4654, 27646,    12,  5144, 11090, 40911,   326,    12,   278,   347,\n",
      "          6674,   258,   312, 10323,    14, 22232,   436, 15973,   372,   312,\n",
      "          7651,   405,   284,  1633,   258,   301,   312,   298, 39113,   496,\n",
      "         24085, 14771,  1119,    14]], device='cuda:0')\n",
      "\n",
      "Texto reconstruido:\n",
      " Es importante tener en cuenta las medidas de higiene en tiempos de pandemia por coronavirus. \n"
     ]
    }
   ],
   "source": [
    "input_ids_corona = tokenizer.encode(texto_corona, return_tensors='pt').to('cuda')\n",
    "\n",
    "\n",
    "texto_reconstruido = tokenizer.decode(tokenizer.encode(texto_corona))\n",
    "\n",
    "print('Identificadores de los tokens:\\n', input_ids)\n",
    "print('\\nTexto reconstruido:\\n', texto_reconstruido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Texto Generado 1\n",
      "\n",
      "Es importante tener en cuenta las medidas de higiene en tiempos de pandemia por coronavirus. \n",
      "El Gobierno estadounidense declaró en abril de 2020 que \"los trabajadores de salud pública estadounidenses deben enfrentar y trabajar en condiciones seguras\" en todo el mundo para que cualquier voluntario que se enfermen se le presione a la salud para mantener la salud.\n",
      "\n",
      "El 12 de junio de 2020, la pandemia de coronavirus en la ONU se incrementó al 31 de marzo, y el gobierno de Estados Unidos anunció que había alcanzado los 12 millones de personas en menos de seis meses. La ciudad de Wuhan, ubicada con la capital, es el centro del país. \n",
      "\n",
      "El 8 de julio del 2020, la Agencia Europea de Servicios de Emergencia fue establecida, con su responsabilidad, controlar el aislamiento de Wuhan del estado de emergencia. El 14 de julio se había establecido el toque de queda en todo el territorio continental.\n",
      "\n",
      "Las escuelas, las cárceles de salud y otros centros de salud fueron cerrados y el uso de medios públicos se redujo a cero.\n",
      "\n",
      "Cada ciudad en el extranjero tiene un servicio de emergencias con el mismo fin, que cubre las necesidades de salud. La Autoridad Europea de Servicios de Emergencia administra los servicios públicos en 12 provincias en todo el país.\n",
      "\n",
      "Se requiere\n",
      "\n",
      "---\n",
      ">> Texto Generado 2\n",
      "\n",
      "Es importante tener en cuenta las medidas de higiene en tiempos de pandemia por coronavirus. \n",
      "En el hospital de la villa, hay un servicio gratuito de cuidado de sangre, que ofrece gratuitamente una pequeña muestra pública de su sangre en la calle. \n",
      "La Universidad de Valladolid está ubicada en la zona de su campus principal. \n",
      "\n",
      "El hospital cuenta con un centro de salud local con un centro para consultas y atención domiciliaria. También hay tres laboratorios del Centro Estatal de Salud del Hospital de Valladolid.\n",
      "\n",
      "El centro de salud de Valladolid contaba con un hospital público hasta el año 2000, con capacidad para 442 camas y 470 pacientes con una capacidad máxima de 5.000 personas; es también centro de investigación de riesgo y de los Centros de Enfermedades Neoestelares, como la Clínica del Centro Nacional de Salud del Hospital Universitario de Valladolid, uno de los centros más avanzados del país y el primer centro no hospitalario en España, inaugurado en el año 1991. \n",
      "\n",
      "El hospital cuenta con 3 campus, 2 de los cuales son del centro de salud de la ciudad, y un centro hospitalario. La Facultad de Medicina de Valladolid tiene como centro el resto del centro, el Hospital de Justicia de Valladolid se encuentra en el campus principal y la Clínica San\n",
      "\n",
      "---\n",
      ">> Texto Generado 3\n",
      "\n",
      "Es importante tener en cuenta las medidas de higiene en tiempos de pandemia por coronavirus. \n",
      "Se debe tratar la enfermedad de coronavirus de la piel, la inmunidad (sin contar el uso de medicamentos), la nutrición (en general), el mantenimiento del cuerpo (por ejemplo una combinación de productos naturales como el tabaco y las verduras), el cuidado de la salud (realizada y monitoreada), la conservación del espacio ambiente (por ejemplo las áreas de protección de la naturaleza y el ambiente), y la salud (no sólo de los animales sino también de los humanos). \n",
      "En España se utiliza como tratamiento para controlar otros trastornos de la salud de la población. El tratamiento por prevención de la enfermedad de coronavirus es la terapia de contención, con efectos similares a aquellos de la medicina tradicional. \n",
      "La intervención de medicamentos (eugenología, prevención, intervención farmacológica, biodegradable, antivirales, decomitantes y bactericida) se pueden hacer en zonas con baja incidencia de riesgo de riesgo. En concreto, existe una actividad en España de tratamiento de infecciones de la piel y las heces de las personas y niños mediante la vacunación en adultos y la vacunación en niños con riesgo de enfermedad por la madre. \n",
      "Existen otros programas de asistencia médica\n",
      "\n",
      "---\n",
      "CPU times: user 5.42 s, sys: 17.8 ms, total: 5.44 s\n",
      "Wall time: 5.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sample_outputs = model.generate(input_ids_corona, pad_token_id=0,\n",
    "                                   do_sample=True, \n",
    "                                   max_length=largo_maximo, \n",
    "                                   min_length=largo_minimo,\n",
    "                                   top_k=40,\n",
    "                                   num_return_sequences=3)\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\">> Texto Generado {}\\n\\n{}\".format(i+1, tokenizer.decode(sample_output.tolist())))\n",
    "    print('\\n---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai2",
   "language": "python",
   "name": "fastai2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
